{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9086c85",
   "metadata": {},
   "source": [
    "## Encoding Techniques.\n",
    "\n",
    "In machine learning, encoding techniques are used to convert categorical data (data that represents categories or labels) into numerical format so that machine learning algorithms can process them. Python offers several libraries and techniques for encoding categorical data. Here are some common encoding techniques in ML using Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f269f2",
   "metadata": {},
   "source": [
    "Types of Encoding \n",
    "1. Nominal Encoding: we dont need to rank these catagories of data if we convert it in numaric form.\n",
    "    example: Gender: Male & Female.. we dont need to decide the rank.\n",
    "            \n",
    "2. Ordinal Encoding : in this we need to specify the orde of the data like Degree feature.\n",
    "we have to specify High school , then 12 th then BE then ME etc...\n",
    "\n",
    "Based on this we can specify encoding types :\n",
    "1. Nominal Encoding:\n",
    "    1. One Hot Encoding.\n",
    "    2. One hot Encoding with many Catagories.\n",
    "    3. Mean Encoding.\n",
    "\n",
    "2. Ordinal Encoding:\n",
    "    1. Label Encoding.\n",
    "    2. Target guided ordinal Encoding.\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e795ccb5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Label Encoding:\n",
    "\n",
    "It assigns a unique integer to each category in a categorical feature.\n",
    "It is suitable for ordinal data where the order of categories matters.\n",
    "You can use the LabelEncoder class from the sklearn.preprocessing module.\n",
    " \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce55fcf5",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(categorical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13876477",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example of Label Encoding:\n",
    "\n",
    "Suppose you have a dataset with a categorical feature \"Education Level\" that you want to label encode. Here's how you can do it:\n",
    "\n",
    " \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample data\n",
    "data = {'Education Level': ['High School', 'Bachelor\\'s Degree', 'Associate\\'s Degree', 'Master\\'s Degree', 'High School']}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "df['Encoded Education Level'] = encoder.fit_transform(df['Education Level'])\n",
    "\n",
    "# Display the encoded DataFrame\n",
    "print(df)\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    " \n",
    "    Education Level  Encoded Education Level\n",
    "0       High School                        2\n",
    "1  Bachelor's Degree                       0\n",
    "2  Associate's Degree                      1\n",
    "3   Master's Degree                        3\n",
    "4       High School                        2\n",
    "\n",
    "\n",
    "In this example, label encoding assigns a unique integer to each category in the \"Education Level\" column based on their order of appearance in the dataset. \"Bachelor's Degree\" is assigned 0, \"Associate's Degree\" is assigned 1, \"High School\" is assigned 2, and \"Master's Degree\" is assigned 3.\n",
    "\n",
    "Remember that while label encoding is suitable for ordinal data, it may introduce unintended ordinal relationships if applied to nominal data, so it's crucial to understand the nature of your data before choosing this encoding technique. If there's no inherent order among categories, consider using one-hot encoding instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ebb53",
   "metadata": {},
   "source": [
    "Rank with higher value in label encoding will have the first priority get selected by the ML Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320b825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7fecf65",
   "metadata": {},
   "source": [
    "### One-Hot Encoding:\n",
    "\n",
    "For example i have a feature of Country names.\n",
    "            Germany     France    spain\n",
    "State          1          0         0\n",
    "Geremany       0          1         0\n",
    "France         0          1         0\n",
    "Spain          0          0         1\n",
    "\n",
    "Dummy Variable Trap concept is used in one hot encoding .\n",
    "which means "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c0fac",
   "metadata": {},
   "source": [
    "####  Dummy variable Trap in One Hot Encoding:\n",
    "\n",
    "The dummy variable trap is a situation that can occur when using one-hot encoding to represent categorical data with binary columns. It happens when one binary column can be predicted from the values of the other binary columns in the dataset. In other words, it creates multicollinearity in your dataset, which can lead to issues in some statistical models.\n",
    "\n",
    "Let's break down the dummy variable trap with an example:\n",
    "\n",
    "Suppose you have a categorical feature \"Season\" with three categories: \"Spring,\" \"Summer,\" and \"Fall.\" If you apply one-hot encoding without considering the trap, you might end up with the following columns:\n",
    "\n",
    "Season_Spring\n",
    "Season_Summer\n",
    "Season_Fall\n",
    "\n",
    "Now, consider this: if you know the values of Season_Summer and Season_Fall, you can easily infer the value of Season_Spring. In other words, one of the columns can be predicted from the values of the others. This redundancy can cause issues in some machine learning models, particularly in linear regression, as it violates the assumption of no perfect multicollinearity.\n",
    "\n",
    "To avoid the dummy variable trap, you should use one less binary column than the number of categories. In the example above, you would only need two columns instead of three. For instance:\n",
    "\n",
    "Season_Summer\n",
    "Season_Fall\n",
    "This way, if both Season_Summer and Season_Fall are 0, you can safely assume that the season is \"Spring.\" By omitting one column, you eliminate the multicollinearity issue.\n",
    "\n",
    "In Python, libraries like pandas automatically handle the dummy variable trap when you use the drop_first=True parameter in functions like pd.get_dummies(). This parameter removes one of the binary columns to avoid multicollinearity. Here's an example:\n",
    "\n",
    " \n",
    "import pandas as pd\n",
    "\n",
    "data = {'Season': ['Spring', 'Summer', 'Fall', 'Spring', 'Summer']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    " Apply one-hot encoding with drop_first=True\n",
    "df_encoded = pd.get_dummies(df, columns=['Season'], drop_first=True)\n",
    "The resulting DataFrame will have only two columns: Season_Summer and Season_Fall, avoiding the dummy variable trap.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d129d8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "It creates binary columns for each category in a categorical feature, with a 1 indicating the presence of the category and 0 otherwise.\n",
    "It is suitable for nominal data where there is no inherent order among categories.\n",
    "You can use the OneHotEncoder class from sklearn.preprocessing or the get_dummies function from pandas.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0fadf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(categorical_data.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e27fbd",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881ab39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd5fed34",
   "metadata": {},
   "source": [
    "### Binary Encoding:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18d6814",
   "metadata": {},
   "source": [
    "It combines label encoding and one-hot encoding by first converting categories to integers and then to binary code.\n",
    "It reduces dimensionality compared to one-hot encoding.\n",
    "You can use the BinaryEncoder class from the category_encoders library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43183b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "encoder = BinaryEncoder(cols=['categorical_feature'])\n",
    "encoded_data = encoder.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d35802",
   "metadata": {},
   "source": [
    "### Count Encoding:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94cdac",
   "metadata": {},
   "source": [
    "It replaces each category with the count of its occurrences in the dataset.\n",
    "It can be useful for high cardinality categorical features.\n",
    "You can use the CountEncoder class from the category_encoders library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import CountEncoder\n",
    "\n",
    "encoder = CountEncoder(cols=['categorical_feature'])\n",
    "encoded_data = encoder.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c23d3",
   "metadata": {},
   "source": [
    "### Target Encoding (Mean Encoding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5b4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d924cd6b",
   "metadata": {},
   "source": [
    "It replaces each category with the mean of the target variable for that category.\n",
    "It can be useful for classification problems.\n",
    "You can manually calculate and apply target encoding or use libraries like category_encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd140ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "encoder = TargetEncoder(cols=['categorical_feature'])\n",
    "encoded_data = encoder.fit_transform(df, target_column)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
